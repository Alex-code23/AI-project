# CS 285 : RL with Sequence Models (Lecture 21)

Ce document r√©sume le cours sur l'utilisation des **Mod√®les de S√©quence** en RL.
Jusqu'√† pr√©sent, nous supposions souvent un MDP o√π l'√©tat $s_t$ est enti√®rement observable. Ce cours traite des cas o√π l'agent ne voit qu'une observation partielle $o_t$ (POMDP) et doit m√©moriser le pass√©. Il couvre √©galement l'utilisation d'architectures comme les RNNs (LSTMs) et les Transformers pour r√©soudre ces probl√®mes, ainsi que l'application du RL au langage.

## üå´Ô∏è 1. Au-del√† des MDPs : POMDPs

Dans de nombreux probl√®mes r√©els (robotique, poker, dialogue), l'agent ne conna√Æt pas l'√©tat complet du monde.
* **Observation ($o_t$) :** Ce que l'agent per√ßoit (ex: image cam√©ra).
* **√âtat ($s_t$) :** La configuration r√©elle du monde.
* **Historique ($h_t$) :** La s√©quence des observations pass√©es $(o_1, a_1, \dots, o_t)$.

Dans un POMDP (Partially Observed MDP), la politique doit d√©pendre de l'historique complet, pas juste de la derni√®re observation : $\pi(a_t | h_t)$.

### Solutions Architecturales
1.  **Windowing (Fen√™trage) :** Empiler les $k$ derni√®res images (ex: Atari DQN utilise 4 frames). [cite_start]Simple mais limit√© √† une m√©moire courte[cite: 5].
2.  **RNNs / LSTMs :** Maintenir un √©tat cach√© r√©current $h_t = f(h_{t-1}, o_t)$ qui r√©sume tout le pass√©. [cite_start]C'est l'approche standard pour les POMDPs complexes[cite: 5].

---

## üîÑ 2. Entra√Æner des Politiques R√©currentes

[cite_start]L'utilisation de RNNs (Recurrent Neural Networks) en RL pose des d√©fis techniques sp√©cifiques, notamment pour le stockage et l'entra√Ænement[cite: 14].

### D√©fis du Replay Buffer
* Pour entra√Æner un RNN, on a besoin de s√©quences temporelles, pas de transitions isol√©es $(s, a, r, s')$.
* **Probl√®me :** Si on stocke des s√©quences enti√®res, l'√©tat cach√© initial $h_{init}$ du RNN stock√© dans le buffer est "p√©rim√©" (il a √©t√© g√©n√©r√© par une vieille version des poids).
* **Solutions :**
    1.  **Zero Start :** Toujours initialiser $h_0 = 0$ et r√©-ex√©cuter toute la s√©quence (co√ªteux).
    2.  **Burn-in :** Utiliser une partie de la s√©quence juste pour "chauffer" l'√©tat cach√© avant de commencer √† calculer les gradients.
    3.  [cite_start]**Stored State (R2D2) :** Stocker l'√©tat cach√© $h_t$ dans le buffer, mais accepter qu'il soit l√©g√®rement incorrect (off-policyness de l'√©tat cach√©)[cite: 16].

### Architectures Distribu√©es (IMPALA / R2D2)
Pour passer √† l'√©chelle (ex: DOTA 2, StarCraft), on d√©couple la collecte de donn√©es (Actors) de l'apprentissage (Learner).
* [cite_start]**IMPALA (V-trace) :** Corrige le d√©calage de politique (Lag) entre les acteurs et l'appreneur via des corrections d'Importance Sampling sophistiqu√©es[cite: 16].

---

## üìú 3. RL comme Mod√©lisation de S√©quence (Transformers)

Plut√¥t que d'utiliser des RNNs avec Bellman/TD-learning, on peut utiliser des **Transformers** (comme GPT) pour mod√©liser la distribution des trajectoires compl√®tes.

### Decision Transformer (DT)
[cite_start]On traite le RL comme un probl√®me de pr√©diction de s√©quence supervis√© (Autoregressive Modeling)[cite: 22].
La s√©quence d'entr√©e est :
$$\tau = (\hat{R}_1, s_1, a_1, \hat{R}_2, s_2, a_2, \dots)$$
O√π $\hat{R}_t$ est le **Return-to-go** (somme des r√©compenses futures d√©sir√©es).

* **Entra√Ænement :** Pr√©dire l'action $a_t$ sachant le contexte pass√© et le retour cible $\hat{R}_t$ (Cross-entropy loss).
* **Inf√©rence :** On donne √† l'agent un retour cible √©lev√© (ex: +1000) et il g√©n√®re les actions qui m√®nent statistiquement √† ce retour.
* **Avantage :** Pas de probl√®mes d'instabilit√© li√©s au bootstrapping ou aux Q-values surestim√©es. Stable comme du Supervised Learning.

### Meta-RL et In-Context Learning
Les mod√®les de s√©quence peuvent effectuer du "Meta-RL" implicite. [cite_start]En lisant l'historique de l'√©pisode courant (actions, r√©compenses), le Transformer "comprend" la t√¢che et adapte sa strat√©gie sans mettre √† jour ses poids (In-Context Learning)[cite: 18].

---

## üó£Ô∏è 4. RL pour le Langage (Language Models)

Le langage est le domaine par excellence des mod√®les de s√©quence. Le RL est utilis√© pour finetuner les mod√®les de langage (LLMs) au-del√† de la simple pr√©diction de mot suivant.

### Dialogue comme un POMDP
Une conversation est un processus s√©quentiel o√π l'√©tat interne de l'interlocuteur est cach√©.
* **Action :** Un mot (Token) ou une phrase (Utterance).
* [cite_start]**R√©compense :** Sentiment humain, succ√®s de la n√©gociation, clic, etc.[cite: 31].

### Offline RL pour le Langage
Souvent, on ne peut pas faire interagir un chatbot en live pour apprendre (risque de toxicit√©, lenteur). On utilise l'Offline RL sur des logs de conversations.
* **IQL (Implicit Q-Learning) pour le texte :** Apprend une Value Function sur le vocabulaire. [cite_start]Permet de filtrer les r√©ponses toxiques ou de faible qualit√© tout en restant proche des donn√©es r√©alistes[cite: 32].
* **CHAI (Confidence-Harnessed Adversarial Imitation) :** Utilise des mod√®les pour distinguer le bon langage du mauvais et guider la g√©n√©ration.

---

## ‚úÖ R√©sum√© Technique

| Approche | Architecture | Gestion du pass√© | Avantages | Inconv√©nients |
| :--- | :--- | :--- | :--- | :--- |
| **Frame Stacking** | CNN / MLP | Fen√™tre fixe (ex: 4 images) | Simple, compatible avec tout algo RL. | M√©moire tr√®s courte, rate les d√©pendances longues. |
| **Recurrent RL** | LSTM / GRU | √âtat cach√© $h_t$ | M√©moire infinie (th√©orique), standard pour POMDP. | Difficile √† entra√Æner (BPTT), gestion complexe du Replay Buffer. |
| **Decision Transformer** | Transformer | Attention sur toute la s√©quence | Tr√®s stable, g√®re les d√©pendances longues, pas de TD-error. | Ne peut pas "inventer" une strat√©gie meilleure que la meilleure d√©mo (pas de stitching optimal). |
| **RLHF (Language)** | Transformer | Finetuning via Reward Model | Permet d'aligner les LLMs sur l'intention humaine. | Co√ªteux (collecte de pr√©f√©rences humaines). |

---
[cite_start]*Source: CS 285 Lecture 21 Slides, Instructor: Sergey Levine, UC Berkeley.* [cite: 1]