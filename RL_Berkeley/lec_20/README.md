# CS 285 : Inverse Reinforcement Learning (Lecture 20)

Ce document r√©sume le cours sur l'**Inverse Reinforcement Learning (IRL)**.
Jusqu'√† pr√©sent, nous avons suppos√© que la fonction de r√©compense $r(s,a)$ √©tait donn√©e. Cependant, concevoir des r√©compenses est difficile et sujet √† des effets secondaires ind√©sirables ("reward hacking"). L'IRL vise √† **apprendre la r√©compense** √† partir de d√©monstrations d'un expert, pour ensuite utiliser le RL pour trouver une politique optimale.

## üéØ Motivation : Pourquoi l'IRL ?

1.  **Imitation (Behavioral Cloning) :** Copier simplement les actions de l'expert (Apprentissage Supervis√©) fonctionne mal √† cause de l'accumulation d'erreurs (Distribution Shift). [cite_start]L'agent ne sait pas *pourquoi* l'expert agit ainsi[cite: 1].
2.  **Inf√©rence de l'Intention :** L'IRL tente de comprendre le but sous-jacent (la r√©compense). [cite_start]Si on conna√Æt la r√©compense, on peut trouver une politique qui g√©n√©ralise mieux et qui est robuste aux perturbations[cite: 1].

## üß† Le Principe du Maximum Entropy IRL

L'hypoth√®se centrale est que les d√©monstrations de l'expert sont des √©chantillons tir√©s d'une distribution optimale (ou sous-optimale Boltzmann). On utilise le mod√®le probabiliste vu au cours 19 :

$$p(\tau) \propto \exp(R(\tau))$$

O√π $R(\tau) = \sum_t r(s_t, a_t)$. [cite_start]L'objectif est de trouver les param√®tres $\psi$ de la r√©compense $r_\psi$ qui maximisent la vraisemblance des trajectoires de l'expert $\mathcal{D}_{demo} = \{\tau_i\}$[cite: 1].

### L'Objectif MaxEnt
$$\max_\psi \sum_{\tau \in \mathcal{D}_{demo}} \log p_{r_\psi}(\tau)$$
$$\log p_{r_\psi}(\tau) = R_\psi(\tau) - \log Z$$
O√π $Z = \int \exp(R_\psi(\tau)) d\tau$ est la fonction de partition (tr√®s difficile √† calculer).

### Feature Matching
Si la r√©compense est lin√©aire par rapport √† des caract√©ristiques $\mathbf{f}(\tau)$ (soit $R(\tau) = \mathbf{w}^T \mathbf{f}(\tau)$), alors le gradient de la log-vraisemblance m√®ne √† une propri√©t√© √©l√©gante :
$$\nabla_\mathbf{w} \mathcal{L} = E_{\tau \sim \text{expert}} [\mathbf{f}(\tau)] - E_{\tau \sim \pi_{learned}} [\mathbf{f}(\tau)]$$
[cite_start]L'algorithme converge quand les **comptes de caract√©ristiques (feature counts)** de l'agent correspondent √† ceux de l'expert[cite: 1].

---

## üöÄ Algorithmes Modernes & Deep IRL

Le calcul de la fonction de partition $Z$ n√©cessite de r√©soudre le probl√®me de RL complet (Soft Value Iteration) √† chaque √©tape d'optimisation de la r√©compense ("boucle interne"), ce qui est tr√®s co√ªteux.

### 1. Guided Cost Learning (GCL)
Pour passer √† l'√©chelle avec des r√©seaux de neurones profonds :
* On utilise l'**Importance Sampling** pour estimer $Z$ sans tout r√©-optimiser √† chaque fois.
* On g√©n√®re des √©chantillons avec la politique actuelle $q(\tau)$ pour estimer l'int√©grale.
* [cite_start]Cela revient √† entra√Æner la r√©compense pour donner un score √©lev√© aux d√©mos expertes et un score faible aux √©chantillons g√©n√©r√©s par la politique actuelle[cite: 1].

### 2. Generative Adversarial Imitation Learning (GAIL)
[cite_start]Il existe une connexion forte entre GCL et les **GANs** (Generative Adversarial Networks)[cite: 1].
* **Discriminateur ($D$) :** Essaie de distinguer les √©tats/actions de l'expert (Vrai) de ceux de l'agent (Faux).
* **G√©n√©rateur ($\pi$) :** L'agent (la politique) essaie de tromper le discriminateur.

Au lieu d'apprendre une fonction de r√©compense explicite $r_\psi$, on utilise le discriminateur comme r√©compense imm√©diate :
$$r(s,a) = \log D(s,a) - \log(1 - D(s,a))$$
L'agent RL maximise cette r√©compense, ce qui le force √† imiter la distribution d'√©tats de l'expert.

---

## üèóÔ∏è Structure d'un Algorithme IRL G√©n√©ral

[cite_start]La plupart des algorithmes suivent cette boucle it√©rative[cite: 1]:

1.  **Collecte de Donn√©es :** L'agent ex√©cute sa politique $\pi$ pour g√©n√©rer des trajectoires.
2.  **Mise √† jour de la R√©compense :** On ajuste $r_\psi$ pour qu'elle donne un score plus √©lev√© aux d√©mos de l'expert qu'aux trajectoires g√©n√©r√©es par l'agent.
    * *En MaxEnt IRL :* Monter le gradient de vraisemblance.
    * *En GAIL :* Mettre √† jour le discriminateur.
3.  **Mise √† jour de la Politique :** On utilise un algorithme de RL (ex: Policy Gradient, TRPO, SAC) pour maximiser la nouvelle r√©compense $r_\psi$.

---

## ‚úÖ Avantages et ‚ùå Inconv√©nients

| Avantages | Inconv√©nients |
| :--- | :--- |
| **G√©n√©ralisation :** Apprendre la r√©compense permet de s'adapter √† de nouveaux environnements (Transfer Learning) mieux que le simple clonage. | **Ambigu√Øt√© :** Plusieurs fonctions de r√©compense peuvent expliquer le m√™me comportement (ex: $R=0$ explique tout). N√©cessite souvent de la r√©gularisation (MaxEnt). |
| **Robustesse :** "Lisse" les erreurs de d√©monstration en cherchant l'intention optimale plut√¥t que de copier le bruit. | **Co√ªt de Calcul :** N√©cessite une boucle interne de RL. On r√©sout un MDP √† chaque √©tape d'apprentissage de la r√©compense. |
| **Moins de Donn√©es :** Souvent plus efficace en nombre de d√©mos que le clonage comportemental pur. | **Instabilit√© :** Comme pour les GANs, l'entra√Ænement adversarial (GAIL) peut √™tre instable. |

---
*Source: CS 285 Lecture 20 Slides, Instructor: Sergey Levine, UC Berkeley.*