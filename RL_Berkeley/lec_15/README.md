# CS 285 : Offline Reinforcement Learning (Lecture 15)

Ce document r√©sume le cours sur l'**Offline RL** (aussi appel√© Batch RL).
Contrairement au RL "Online" (qui collecte des donn√©es en apprenant) ou au RL "Off-policy" (qui utilise un buffer pass√© mais continue d'explorer), l'Offline RL doit apprendre une politique √† partir d'un **dataset statique fixe** $\mathcal{D}$, sans jamais pouvoir interagir avec l'environnement pour tester ou corriger ses hypoth√®ses.

## ‚ö†Ô∏è Le Probl√®me Fondamental : Distribution Shift & OOD Actions

Si on applique un algorithme classique (DQN ou SAC) sur un dataset statique, il √©choue catastrophiquement. Pourquoi ?

### 1. Counterfactual Queries (Requ√™tes Contrefactuelles)
L'algorithme de Q-Learning cherche √† maximiser la valeur :
$$y = r + \gamma \max_{a'} Q(s', a')$$
Pour calculer la cible, l'algorithme interroge la Q-function sur des actions $a'$ qui maximisent $Q$. Or, ces actions $a'$ ne sont souvent **pas pr√©sentes dans le dataset** (elles sont "Out-Of-Distribution" ou OOD).

### 2. Overestimation & Exploitation
Comme la Q-function n'a jamais vu ces actions OOD lors de l'entra√Ænement, elle pr√©dit des valeurs arbitraires (souvent bruit√©es). L'op√©rateur $\max$ s√©lectionne syst√©matiquement ces erreurs positives (hallucinations). L'agent pense avoir trouv√© une strat√©gie miracle, alors qu'il exploite simplement les zones d'ombre du mod√®le.

---

## üõ†Ô∏è Solutions Algorithmiques

L'objectif de l'Offline RL est de rester "proche" des donn√©es pour √©viter les zones inconnues, tout en essayant de faire mieux que la politique qui a g√©n√©r√© les donn√©es ($\pi_\beta$).

### 1. Contraintes de Politique (Policy Constraints)
L'id√©e est de forcer la politique apprise $\pi_\theta$ √† rester proche de la politique comportementale (Behavior Policy) $\pi_\beta$ (celle qui a g√©n√©r√© le dataset).

**Formulation :**
$$\pi_\theta = \arg\max_\pi E_{(s,a) \sim \mathcal{D}} [Q_\phi(s, a)] \quad \text{s.t.} \quad D(\pi_\theta, \pi_\beta) \le \epsilon$$

* **D√©fis :** On ne conna√Æt pas $\pi_\beta$ explicitement (on a juste des √©chantillons). Il faut souvent l'estimer (Behavior Cloning).
* **Algorithmes :**
    * **BCQ (Batch-Constrained Q-learning) :** G√©n√®re des actions candidates via un VAE (entra√Æn√© sur le dataset) et s√©lectionne la meilleure selon $Q$.
    * **BEAR (Bootstrapping Error Accumulation Reduction) :** Utilise le "Support Matching" (MMD) plut√¥t que la divergence KL. L'agent peut choisir n'importe quelle action tant qu'elle a une probabilit√© non-nulle dans le dataset.

### 2. M√©thodes Conservatrices (Conservative Q-Learning - CQL)
Au lieu de contraindre l'acteur ($\pi$), on modifie le critique ($Q$) pour qu'il soit **pessimiste** sur les actions inconnues.

**Principe :**
On ajoute un terme de r√©gularisation √† la fonction de perte de Q-Learning pour **minimiser** la valeur des actions choisies par la politique actuelle, et **maximiser** la valeur des actions r√©elles du dataset.

$$\mathcal{L}(\theta) = \underbrace{\text{Standard Bellman Error}}_{\text{Fitting data}} + \alpha (\underbrace{E_{a \sim \pi}[Q(s,a)]}_{\text{Minimize policy actions}} - \underbrace{E_{a \sim \pi_\beta}[Q(s,a)]}_{\text{Maximize data actions}})$$

* **R√©sultat :** La Q-function apprend une **borne inf√©rieure** (Lower Bound) de la vraie valeur. On est garanti de ne pas surestimer, ce qui rend l'optimisation s√ªre.

### 3. Model-Based Offline RL (MOPO / MOREL)
Si on apprend un mod√®le de la dynamique $T(s'|s,a)$ sur le dataset, on rencontre le m√™me probl√®me : le mod√®le va halluciner des √©tats futurs optimistes pour les actions OOD.

**Solution :**
1.  Apprendre un mod√®le dynamique (souvent un ensemble pour estimer l'incertitude).
2.  P√©naliser la r√©compense par l'incertitude du mod√®le :
    $$r(s, a) = \hat{r}(s, a) - \lambda \cdot u(s, a)$$
    O√π $u(s,a)$ est la variance des pr√©dictions de l'ensemble.
3.  Planifier ou apprendre une politique dans ce MDP p√©nalis√© (MDP Pessimiste).

---

## ‚öñÔ∏è Comparaison des Approches

| Approche | M√©canisme | Avantages | Inconv√©nients |
| :--- | :--- | :--- | :--- |
| **Importance Sampling** | Reweighting des retours | Th√©oriquement sans biais. | Variance exponentielle avec l'horizon (inutilisable en pratique pour l'entra√Ænement). |
| **Policy Constraints (BCQ, BEAR)** | Restreindre $\pi$ √† $\pi_\beta$ | Conceptuellement intuitif. | N√©cessite d'estimer $\pi_\beta$ (Behavior Cloning), ce qui est difficile et source d'erreurs. |
| **Conservative Q (CQL)** | Apprendre une Q-function pessimiste | Tr√®s robuste, SOTA, pas besoin d'estimer $\pi_\beta$. | Peut √™tre trop conservateur (sous-performance) si $\alpha$ est trop grand. |
| **Model-Based (MOPO)** | P√©naliser l'incertitude du mod√®le | G√©n√©ralise mieux hors du dataset si la physique est simple. | Difficile si la dynamique est complexe (images). |

---

## üîë R√©sum√© : Pourquoi l'Offline RL est dur ?
1.  **Pas de correction possible :** L'agent ne peut pas essayer une action pour voir "si √ßa marche vraiment".
2.  **Maximisation biais√©e :** L'optimiseur cherche les erreurs du mod√®le (OOD) et les exploite.
3.  **Compromis Conservatisme/Performance :** Si on reste trop proche des donn√©es (Behavior Cloning), on ne s'am√©liore pas. Si on s'√©loigne trop, on plante. L'art de l'Offline RL est de trouver la limite de g√©n√©ralisation s√ªre.

---
*Source: CS 285 Lecture 15 Slides, Instructor: Sergey Levine, UC Berkeley.*