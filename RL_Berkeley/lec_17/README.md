# CS 285 : Reinforcement Learning Theory Basics (Lecture 17)

Ce document r√©sume le cours sur les **Fondements Th√©oriques du RL**.
Jusqu'√† pr√©sent, nous avons vu des algorithmes qui fonctionnent "empiriquement" (Deep RL). Ici, on cherche √† √©tablir des bornes formelles sur la performance et la vitesse d'apprentissage, principalement dans le cadre tabulaire (√©tats discrets finis) ou lin√©aire.

## üéØ Les Questions Fondamentales

La th√©orie du RL cherche principalement √† r√©pondre √† deux questions :

1.  **Sample Complexity (Complexit√© en √âchantillons) :** Combien de pas de temps $N$ faut-il pour trouver une politique $\pi$ qui est $\epsilon$-proche de l'optimale ($\pi^*$) ?
    $$J(\pi^*) - J(\hat{\pi}) \le \epsilon \quad \text{avec probabilit√© } 1-\delta$$
2.  **Regret :** Quelle est la perte cumul√©e subie par l'agent pendant qu'il apprend (par rapport √† un agent optimal) ?
    $$Reg(T) = \sum_{t=1}^T (J(\pi^*) - J(\pi_t))$$
    On cherche souvent un regret "sous-lin√©aire" (ex: $\sqrt{T}$), ce qui signifie que l'agent finit par converger vers l'optimal.

---

## üèóÔ∏è 1. Model-Based RL (Analyse Tabulaire)

L'approche la plus simple √† analyser est le Model-Based :
1.  Estimer le mod√®le de transition $\hat{T}(s'|s,a)$ et la r√©compense $\hat{r}(s,a)$ par comptage empirique.
2.  Planifier sur ce mod√®le estim√© (ex: Value Iteration).

### Simulation Lemma (Le Lemme de Simulation)
Ce lemme fondamental relie l'erreur du mod√®le √† l'erreur de valeur.
Si notre mod√®le a une erreur de pr√©diction $\epsilon_m$, l'erreur sur la valeur de la politique apprise est born√©e par :
$$|V^\pi(s) - \hat{V}^\pi(s)| \le \frac{\gamma}{(1-\gamma)^2} \epsilon_m$$

* **Impact :** L'erreur est amplifi√©e quadratiquement par l'horizon effectif $\frac{1}{1-\gamma}$. Une petite erreur de mod√®le peut ruiner la politique √† long terme.

### Exploration Optimiste (MBIE-EB / UCRL)
Pour garantir la convergence, il ne suffit pas d'apprendre un mod√®le moyen. Il faut √™tre **optimiste**.
Au lieu d'utiliser le mod√®le moyen $\hat{T}$, on construit un ensemble de mod√®les plausibles (Confidence Set) et on choisit celui qui maximise la valeur.
En pratique, cela revient √† ajouter un **bonus d'exploration** aux r√©compenses :
$$r^+(s,a) = \hat{r}(s,a) + \frac{C}{\sqrt{N(s,a)}}$$
Cela garantit (avec haute probabilit√©) que $Q^+(s,a) \ge Q^*(s,a)$.

---

## ‚ö° 2. Model-Free RL (Q-Learning)

Peut-on avoir des garanties similaires sans apprendre de mod√®le ?
Oui, pour des algorithmes comme **Q-Learning avec UCB**.

### Lower Bounds (Bornes Inf√©rieures)
On ne peut pas apprendre plus vite que la th√©orie de l'information ne le permet. Pour un MDP tabulaire, tout algorithme a besoin d'au moins $\Omega\left(\frac{|S||A|}{\epsilon^2 (1-\gamma)^3}\right)$ √©chantillons pour trouver une politique $\epsilon$-optimale.

### Upper Bounds (Bornes Sup√©rieures)
Les algorithmes modernes (comme UCB-VI ou Q-learning optimiste) atteignent des performances proches de cette limite optimale.
* **Id√©e cl√© :** Ajouter un bonus $\frac{1}{\sqrt{N(s,a)}}$ directement dans la mise √† jour de Q-Learning.

---

## üìâ 3. Function Approximation & Offline RL

Quand on passe aux r√©seaux de neurones (Deep RL), les garanties deviennent plus floues.

### Approximation Lin√©aire
Si la Q-function est lin√©aire ($Q(s,a) = \theta^T \phi(s,a)$), on peut prouver la convergence si les donn√©es sont bien distribu√©es.

### Le D√©fi de l'Offline RL (Distribution Shift)
En Offline RL, la th√©orie se concentre sur le **Concentrability Coefficient** ($C$).
Il mesure le ratio de densit√© entre la politique que l'on veut apprendre ($\pi$) et la politique qui a g√©n√©r√© les donn√©es ($\mu$).
$$C \approx \max_{s,a} \frac{d^\pi(s,a)}{d^\mu(s,a)}$$
* Si ce ratio est born√© partout (nos donn√©es couvrent tout ce que $\pi$ pourrait visiter), on peut apprendre.
* Si ce ratio explose (il y a des √©tats que $\pi$ visite mais que $\mu$ n'a jamais vus), l'erreur peut √™tre arbitrairement grande. C'est la justification th√©orique des algorithmes conservateurs (CQL) vus au cours 15/16.

---

## ‚úÖ R√©sum√© des Concepts Th√©oriques

| Concept | D√©finition | Importance |
| :--- | :--- | :--- |
| **Simulation Lemma** | Relie l'erreur de mod√®le √† l'erreur de valeur. | Montre pourquoi l'horizon long ($1/1-\gamma$) rend l'apprentissage difficile. |
| **Optimism (UCB)** | Agir comme si l'environnement √©tait le "meilleur possible" compatible avec les donn√©es. | Indispensable pour une exploration provable (garantie). L'al√©atoire ($\epsilon$-greedy) ne suffit pas. |
| **Sample Complexity** | Nombre d'√©chantillons n√©cessaires pour apprendre. | G√©n√©ralement proportionnel au nombre d'√©tats $|S|$ et d'actions $|A|$. |
| **Concentrability** | Ratio entre la distribution cible et la distribution des donn√©es. | Condition sine qua non pour la r√©ussite de l'Offline RL. |

---
*Source: CS 285 Lecture 17 Slides, Instructor: Sergey Levine, UC Berkeley.*