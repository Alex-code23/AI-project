# CS 285 : Exploration Part 2 - Inference & Prediction (Lecture 14)

Ce document r√©sume la deuxi√®me partie du cours sur l'**Exploration**.
Alors que la partie 1 se concentrait sur le comptage des √©tats (Count-Based), cette partie explore des m√©thodes plus g√©n√©rales bas√©es sur l'**erreur de pr√©diction** (la curiosit√©) et la maximisation de l'**information mutuelle** (l'acquisition de comp√©tences).

## üîÆ 1. Erreur de Pr√©diction & Curiosit√© (Curiosity-Based Exploration)

L'intuition est simple : si le mod√®le est surpris par une transition, c'est que l'√©tat est nouveau ou mal compris. On utilise l'erreur de pr√©diction comme signal de r√©compense intrins√®que.

### Le Probl√®me du "TV Blanc" (The Noisy TV Problem)
Si on utilise l'erreur de pr√©diction brute sur les pixels ($||I_{t+1} - \hat{I}_{t+1}||^2$) comme r√©compense :
* L'agent sera attir√© par le bruit stochastique impr√©visible (ex: la neige sur un √©cran de t√©l√©, le mouvement des feuilles).
* Il restera bloqu√© √† regarder ce bruit car l'erreur de pr√©diction restera toujours √©lev√©e, m√™me s'il ne peut rien y apprendre ("procrastination").

### Solution : Intrinsic Curiosity Module (ICM)
Pour √©viter ce pi√®ge, on ne pr√©dit pas les pixels bruts, mais une repr√©sentation latente $\phi(s)$ qui ne contient que ce qui est **contr√¥lable** par l'agent.

L'architecture ICM (Pathak et al., 2017) comprend deux sous-modules :
1.  **Inverse Model (Mod√®le Inverse) :** Pr√©dire l'action $a_t$ connaissant $s_t$ et $s_{t+1}$.
    * Cela force $\phi(s)$ √† ne coder que les √©l√©ments de l'environnement sur lesquels l'agent peut agir. Le bruit de fond incontr√¥lable est ignor√©.
2.  **Forward Model (Mod√®le Direct) :** Pr√©dire $\phi(s_{t+1})$ connaissant $\phi(s_t)$ et $a_t$.
    * L'erreur de pr√©diction dans cet espace latent sert de r√©compense intrins√®que :
      $$r_i(s_t, a_t) = || \hat{\phi}(s_{t+1}) - \phi(s_{t+1}) ||^2$$

---

## üß† 2. Maximisation de l'Information (Information Gain)

On veut explorer pour r√©duire notre incertitude sur la dynamique de l'environnement $\theta$.
On cherche √† maximiser le **Gain d'Information** (la r√©duction d'entropie de notre croyance sur $\theta$) :
$$IG(z, y) = H(\theta) - H(\theta | y)$$

### Variational Information Maximization (VIME)
Calculer le gain d'information exact est impossible. VIME (Houthooft et al., 2016) utilise une borne variationnelle :
* On apprend un mod√®le de dynamique Bay√©sien (BNN) $p_\theta(s_{t+1}|s_t, a_t)$.
* Le bonus d'exploration est la divergence KL entre la croyance *a posteriori* (apr√®s avoir vu la transition) et la croyance *a priori* :
  $$r_i(s_t, a_t) \approx D_{KL}(q_{\text{new}}(\theta) || q_{\text{old}}(\theta))$$

---

## üéØ 3. Exploration par Objectifs (Goal-Conditioned RL)

Au lieu d'explorer au hasard, l'agent peut se fixer ses propres objectifs.

### Apprendre DIADYN (DIAYN - Diversity Is All You Need)
On veut apprendre un ensemble de comp√©tences (skills) distinctes sans r√©compense externe.
On maximise l'information mutuelle entre les √©tats visit√©s $S$ et une "comp√©tence" latente $Z$ (un entier ou un vecteur one-hot choisi au d√©but de l'√©pisode).
$$I(S; Z) = H(Z) - H(Z|S)$$

Cela se traduit par deux objectifs :
1.  **Discernabilit√© :** En voyant l'√©tat $s$, on doit pouvoir deviner quelle comp√©tence $z$ l'agent ex√©cutait (via un discriminateur $q_\phi(z|s)$).
2.  **Diversit√© :** Les √©tats visit√©s doivent √™tre aussi vari√©s que possible (maximiser l'entropie des √©tats).

La r√©compense intrins√®que devient : $r_i(s, a) = \log q_\phi(z|s) - \log p(z)$.

### GCRL (Goal-Conditioned RL)
On entra√Æne une politique $\pi(a|s, g)$ capable d'atteindre n'importe quel but $g$.
* **Hindsight Experience Replay (HER) :** M√™me si l'agent rate son but $g$, il a forc√©ment atteint un autre √©tat $s_{final}$. On r√©-√©tiquette cette transition comme une r√©ussite pour le but $g' = s_{final}$. "Je n'ai pas r√©ussi ce que je voulais faire, mais j'ai r√©ussi ce que j'ai fait".

---

## ‚úÖ R√©sum√© des M√©thodes d'Exploration

| M√©thode | Principe | Avantages | Inconv√©nients |
| :--- | :--- | :--- | :--- |
| **ICM (Curiosit√©)** | Maximiser l'erreur de pr√©diction sur les features contr√¥lables (Inverse Model). | Filtre le bruit stochastique (TV Problem). | Peut rater des infos pertinentes mais non contr√¥lables. |
| **VIME (Info Gain)** | Maximiser la r√©duction d'incertitude du mod√®le (KL Divergence). | Th√©oriquement fond√©. | Lourd (n√©cessite BNN), un peu dat√©. |
| **DIAYN (Skills)** | Maximiser l'Information Mutuelle entre √âtat et Comp√©tence. | Apprend des comportements utiles sans aucune r√©compense. | Difficile de transf√©rer ces comp√©tences vers une t√¢che pr√©cise ensuite. |
| **HER (Goals)** | Apprendre de ses √©checs en changeant le but a posteriori. | Extr√™mement efficace pour atteindre des √©tats pr√©cis. | Suppose qu'on peut √©chantillonner des buts dans l'espace d'√©tats. |

---
*Source: CS 285 Lecture 14 Slides, Instructor: Sergey Levine, UC Berkeley.*