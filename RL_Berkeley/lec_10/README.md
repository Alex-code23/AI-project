# CS 285 : Model-Based Reinforcement Learning (Part 1) (Lecture 10)

Ce document r√©sume le cours sur le **Model-Based RL (MBRL)**. Contrairement aux m√©thodes Model-Free (PG, Actor-Critic, Q-Learning) qui apprennent directement une politique ou une valeur, MBRL apprend un **mod√®le de la dynamique** de l'environnement, puis utilise ce mod√®le pour **planifier** ou optimiser une trajectoire.

## üèóÔ∏è Le Principe Fondamental

L'objectif est d'apprendre la fonction de transition $f(s_t, a_t)$ telle que :
$$s_{t+1} = f(s_t, a_t)$$

Une fois ce mod√®le appris (souvent un r√©seau de neurones param√©tr√© par $\phi$), on formule le probl√®me de contr√¥le comme un probl√®me d'optimisation :

$$\arg\max_{a_1, \dots, a_T} \sum_{t=1}^T r(s_t, a_t) \quad \text{sujet √† } s_{t+1} = f_\phi(s_t, a_t)$$

---

## ‚ö†Ô∏è Le Probl√®me du D√©calage de Distribution (Distribution Mismatch)

L'algorithme na√Øf (Version 0.5/1.0) consiste √† collecter des donn√©es al√©atoires, entra√Æner le mod√®le $f_\phi$, puis planifier. Cela √©choue souvent √† cause du **Covariate Shift** :

1.  Le mod√®le est entra√Æn√© sur des donn√©es $p_{\text{train}}(s)$.
2.  La politique planifi√©e induit une nouvelle distribution de visite $p_{\pi}(s)$.
3.  Petite erreur sur $f_\phi$ $\rightarrow$ l'agent visite des √©tats l√©g√®rement diff√©rents $\rightarrow$ le mod√®le ne conna√Æt pas ces √©tats $\rightarrow$ l'erreur explose.

**Th√©orie :** Si le mod√®le a une erreur $\epsilon$ √† chaque pas, l'erreur totale sur la trajectoire cro√Æt en **$O(T^2)$** (quadratique en l'horizon).

---

## üõ†Ô∏è Solutions Algorithmiques

### 1. DAgger pour la Dynamique (Dataset Aggregation)
Pour corriger le d√©calage de distribution, on force le mod√®le √† apprendre sur les √©tats que la politique actuelle visite.
1.  Entra√Æner le mod√®le $f_\phi$ sur le dataset $\mathcal{D}$.
2.  Utiliser le mod√®le pour planifier une politique $\pi_\phi$.
3.  Ex√©cuter $\pi_\phi$ dans le vrai environnement pour g√©n√©rer de nouvelles transitions $(s, a, s')$.
4.  Ajouter ces donn√©es √† $\mathcal{D}$ et recommencer.

### 2. Model-Predictive Control (MPC)
Au lieu d'ex√©cuter toute la s√©quence planifi√©e (Open Loop), on utilise une approche √† **horizon fuyant (Closed Loop)** pour corriger les erreurs du mod√®le en temps r√©el.
1.  Observer l'√©tat $s_t$.
2.  Optimiser la s√©quence d'actions $\{a_t, \dots, a_{t+H}\}$ qui maximise la r√©compense pr√©dite par le mod√®le.
3.  Ex√©cuter **seulement la premi√®re action** $a_t$.
4.  Observer le nouvel √©tat r√©el $s_{t+1}$.
5.  R√©p√©ter.

---

## üß† Optimisation et Planification (Comment choisir les actions ?)

Une fois le mod√®le $f_\phi$ appris, comment trouver la s√©quence d'actions optimale ? On ne peut pas toujours utiliser la descente de gradient (Backpropagation through time) car les gradients explosent/disparaissent sur de longues horizons.

### M√©thodes sans Gradient (Gradient-Free Optimization) :
1.  **Random Shooting :** G√©n√©rer $N$ s√©quences d'actions al√©atoires, √©valuer leur r√©compense cumul√©e avec le mod√®le, choisir la meilleure. (Simple mais inefficace en haute dimension).
2.  **CEM (Cross-Entropy Method) :** M√©thode it√©rative.
    * √âchantillonner des actions depuis une distribution (ex: Gaussienne).
    * S√©lectionner les $K$ meilleures s√©quences ("√©lites").
    * Mettre √† jour la moyenne et la variance de la distribution pour se rapprocher des √©lites.
    * R√©p√©ter.

---

## üîÆ Incertitude et "Model Exploitation"

Les r√©seaux de neurones g√©n√©ralisent mal hors de leur distribution d'entra√Ænement.
**Le probl√®me :** L'optimiseur (le planificateur) va chercher des actions pour lesquelles le mod√®le pr√©dit (√† tort) une r√©compense √©norme ("Model Exploitation"). Le mod√®le "hallucine" des gains.

**La Solution : Estimer l'Incertitude (Epistemic Uncertainty)**
L'agent doit savoir ce qu'il ne sait pas.
* **Bootstrap Ensembles :** Entra√Æner $N$ mod√®les ind√©pendants $f_{\phi_1}, \dots, f_{\phi_N}$ sur les m√™mes donn√©es (avec r√©-√©chantillonnage).
* **Utilisation :** Lors de la planification, on utilise la moyenne des pr√©dictions, ou on p√©nalise les actions o√π les mod√®les sont en d√©saccord (forte variance).

---

## üñºÔ∏è Mod√®les Complexes (Images)

Pour les observations visuelles (pixels), on ne peut pas pr√©dire directement $s_{t+1}$ (vecteur d'√©tat inconnu). On utilise des **Video Prediction Models** (ex: Convolutional LSTM, Stochastic Variational Video Prediction) pour pr√©dire les futures frames, puis on optimise par rapport √† une fonction de r√©compense d√©finie sur les pixels ou un but visuel.

---

## ‚úÖ Avantages et ‚ùå Inconv√©nients du MBRL

| Avantages | Inconv√©nients |
| :--- | :--- |
| **Sample Efficiency :** Extr√™mement efficace. Un mod√®le apprend la physique du monde bien plus vite qu'une politique n'apprend √† maximiser un score. (Ex: 10x √† 100x moins de donn√©es que le Model-Free). | **Complexit√© de calcul :** La planification (MPC/CEM) est co√ªteuse en temps de calcul √† l'ex√©cution (inference time). |
| **Transf√©rabilit√© :** Le mod√®le de dynamique est agnostique √† la t√¢che (reward function). Si la t√¢che change, le mod√®le reste valide. | **Biais Asymptotique :** Si le mod√®le n'est pas parfait, la performance finale sera limit√©e par la qualit√© du mod√®le ("Model Bias"). Le Model-Free finit souvent par √™tre meilleur asymptotiquement. |
| **S√©curit√© :** Permet de pr√©dire des √©tats dangereux avant de les atteindre. | **Model Exploitation :** N√©cessite une bonne gestion de l'incertitude pour √©viter d'exploiter les erreurs du mod√®le. |

---
*Source: CS 285 Lecture 10 Slides, Instructor: Sergey Levine, UC Berkeley.*