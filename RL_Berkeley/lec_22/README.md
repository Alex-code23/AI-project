# CS 285 : Meta-Learning & Transfer Learning (Lecture 22)

Ce document r√©sume le cours sur le **Meta-Learning** (apprendre √† apprendre) et le **Transfer Learning**. Contrairement au RL classique qui apprend chaque t√¢che de z√©ro ("tabula rasa"), ces m√©thodes visent √† utiliser l'exp√©rience acquise sur des t√¢ches pass√©es pour apprendre de nouvelles t√¢ches plus rapidement et plus efficacement.

## üéØ Motivation : Au-del√† du "Tabula Rasa"

Le Deep RL standard est inefficace en termes d'√©chantillons (sample inefficient). Pour r√©soudre une nouvelle t√¢che, il doit tout r√©apprendre.
* **Intuition humaine :** Si vous savez ouvrir une porte, vous savez probablement ouvrir un placard. Si vous savez jouer √† Mario, vous apprendrez Sonic plus vite.
* **Le but :** Utiliser des connaissances pr√©alables (Priors) structurelles ou dynamiques pour acc√©l√©rer l'acquisition de nouvelles comp√©tences.

---

## üîÑ Transfer Learning

L'objectif est d'utiliser l'exp√©rience d'un **domaine source** pour √™tre performant sur un **domaine cible**.

### Terminologie
* **0-shot :** Ex√©cuter la politique entra√Æn√©e directement sur la nouvelle t√¢che sans entra√Ænement suppl√©mentaire.
* **Few-shot :** L'agent a droit √† quelques essais (√©pisodes) sur la nouvelle t√¢che pour s'adapter.
* **Fine-tuning :** Entra√Æner sur la source, puis continuer l'entra√Ænement (avec un learning rate plus bas) sur la cible.

### Pourquoi le Fine-tuning √©choue souvent en RL ?
Contrairement √† la vision par ordinateur (ImageNet), le fine-tuning en RL est difficile :
1.  **Exploration :** Une politique optimale sur la t√¢che source est souvent **d√©terministe**. Elle a "oubli√©" comment explorer. Face √† une nouvelle t√¢che, elle √©choue √† d√©couvrir les nouvelles r√©compenses.
2.  **Sp√©cialisation :** Les repr√©sentations apprises deviennent trop sp√©cifiques √† la dynamique de la t√¢che source.

---

## üß† Meta-Reinforcement Learning (Meta-RL)

Le Meta-RL formule le probl√®me non pas comme "apprendre une t√¢che", mais comme **"apprendre un algorithme d'apprentissage"**.

### Formulation Math√©matique
Si un algorithme d'apprentissage g√©n√©rique s'√©crit $\phi = f_{learn}(\mathcal{D}^{tr})$, le Meta-Learning cherche √† optimiser la fonction $f_\theta$ sur un ensemble de t√¢ches :

$$\theta^* = \arg\max_\theta \sum_{i=1}^n E_{\pi_{\phi_i}(\tau)} [R(\tau)] \quad \text{o√π} \quad \phi_i = f_\theta(\mathcal{M}_i)$$

* $\mathcal{M}_i$ : Une t√¢che (MDP) √©chantillonn√©e depuis une distribution $p(\mathcal{M})$.
* $f_\theta$ : La proc√©dure d'adaptation (le m√©ta-mod√®le).
* $\phi_i$ : Les param√®tres adapt√©s √† la t√¢che $i$.

L'agent doit maximiser la r√©compense cumul√©e sur l'ensemble de l'exp√©rience ("Meta-episode"), ce qui inclut les essais exploratoires et les essais finaux.

---

## üìê Les 3 Perspectives du Meta-RL

Le cours classifie les algorithmes de Meta-RL en trois cat√©gories principales, qui sont math√©matiquement li√©es mais diff√®rent par leur impl√©mentation.

### 1. Perspective R√©currente (RNN / Black-Box)
On utilise un r√©seau de neurones r√©current (RNN, LSTM, Transformer) qui prend en entr√©e toute l'histoire des interactions (√©tats, actions, r√©compenses).

* **Principe :** L'√©tat cach√© $h_i$ du RNN sert de "m√©moire" ou de "param√®tres appris". Le RNN *apprend* √† explorer et √† adapter sa strat√©gie au fil des timesteps sans mise √† jour explicite des poids (les poids $\theta$ du RNN sont fixes au test, c'est l'activit√© interne qui change).
* **Architecture :**
    $$\pi_\theta(a_t | s_t, h_t) \quad \text{o√π} \quad h_{t+1} = \text{RNN}(h_t, s_t, a_t, r_t)$$
    *Crucial :* L'√©tat cach√© $h_t$ n'est **pas r√©initialis√©** entre les √©pisodes d'une m√™me t√¢che.
* **Exemples :** RL2 (Duan et al.), Learning to Reinforcement Learn (Wang et al.).

### 2. Perspective Optimisation (Gradient-Based / MAML)
On force la proc√©dure d'adaptation $f_\theta$ √† √™tre une √©tape de descente de gradient. On cherche des param√®tres initiaux $\theta$ tels qu'un seul pas de gradient sur une nouvelle t√¢che m√®ne √† une politique performante.

* **Algorithme (MAML - Model-Agnostic Meta-Learning) :**
    $$J(\theta) = \sum_i J_i(\theta - \alpha \nabla_\theta J_i(\theta))$$
    On optimise $\theta$ pour que la performance *apr√®s* mise √† jour soit maximale.
* **Avantage :** Mod√®le agnostique, garantit une convergence asymptotique (car c'est toujours du gradient descent).
* **Inconv√©nient :** N√©cessite de calculer des d√©riv√©es secondes (Hessiennes) ou des approximations complexes.

### 3. Perspective Inf√©rence Probabiliste (Task Inference / PEARL)
On consid√®re que la t√¢che est d√©finie par une variable latente cach√©e $z$ (ex: la vitesse cible, la gravit√©). Le probl√®me devient un POMDP (Partially Observed MDP) o√π $z$ doit √™tre inf√©r√©.

* **Principe :** Apprendre une politique conditionn√©e par le contexte $\pi_\theta(a|s, z)$ et un r√©seau d'inf√©rence $q_\phi(z | \text{historique})$.
* **Posterior Sampling (Exploration) :**
    1.  On √©chantillonne une hypoth√®se $z \sim q_\phi(z|\text{context})$.
    2.  On agit selon cette hypoth√®se (exploration structur√©e).
    3.  On met √† jour le contexte avec les nouvelles donn√©es.
* **Exemple :** PEARL (Probabilistic Embeddings for Actor-Critic RL). C'est souvent l'approche la plus efficace pour le RL off-policy.

---

## ‚öñÔ∏è Comparaison des Architectures

| Perspective | Approche | Avantages | Inconv√©nients |
| :--- | :--- | :--- | :--- |
| **RNN (RL2)** | "Just run an RNN" | Conceptuellement simple, facile √† impl√©menter. | Difficile √† optimiser sur de longues s√©quences, "Meta-Overfitting" fr√©quent. |
| **Gradient (MAML)** | Bi-level Optimization | Bonne extrapolation, structure inductive forte (le gradient est toujours bon). | Complexe √† calculer (d√©riv√©es secondes), n√©cessite beaucoup de samples (On-policy). |
| **Inf√©rence (PEARL)** | POMDP / Variable Latente | Exploration efficace (Posterior Sampling), permet l'Off-policy (Sample efficient). | Architecture plus complexe (Encoder + Policy), difficile √† stabiliser. |

---

## üß¨ Ph√©nom√®nes √âmergents

Le Meta-RL est int√©ressant pour les neurosciences car il fait √©merger des comportements complexes sans qu'ils soient explicitement programm√©s :
* **Apprentissage √âpisodique :** Les r√©seaux r√©currents apprennent √† stocker des √©v√©nements en m√©moire pour les r√©utiliser.
* **Raisonnement Causal :** L'agent apprend √† faire des exp√©riences pour d√©duire les r√®gles de l'environnement (inf√©rence causale implicite).

---
*Source: CS 285 Lecture 22 Slides, Instructor: Sergey Levine, UC Berkeley.*