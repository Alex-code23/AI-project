# CS 285 : Model-Based RL Part 2 - Policy Learning (Lecture 11)

Ce document r√©sume la deuxi√®me partie du cours sur le **Model-Based RL**. Alors que la partie 1 se concentrait sur la planification pure (Shooting/MPC) avec un mod√®le, cette partie traite de l'utilisation du mod√®le pour optimiser directement les param√®tres $\theta$ d'une politique $\pi_\theta(a|s)$.

## üéØ Objectif : Apprendre une Politique via le Mod√®le

Au lieu de replanifier √† chaque pas de temps (ce qui est co√ªteux), nous voulons "distiller" la connaissance du mod√®le dans un r√©seau de neurones rapide et g√©n√©ralisable : la politique $\pi_\theta$.

Trois grandes approches sont abord√©es :
1.  **Backpropagation :** Diff√©rentier analytiquement √† travers le mod√®le dynamique.
2.  **Model-Free avec Mod√®le (Dyna) :** Utiliser le mod√®le pour g√©n√©rer des donn√©es synth√©tiques.
3.  **Mod√®les Locaux & Guided Policy Search :** Utiliser des mod√®les simples (lin√©aires) localement pour guider une politique globale complexe.

---

## 1. Backpropagation √† travers le Mod√®le

Puisque le mod√®le de dynamique $s_{t+1} = f_\phi(s_t, a_t)$ est souvent un r√©seau de neurones, il est **diff√©rentiable**.
On peut calculer le gradient de la somme des r√©compenses directement par rapport aux param√®tres de la politique $\theta$ en utilisant la r√®gle de la cha√Æne (Chain Rule) √† travers le temps.

### Le Probl√®me
Calculer $\frac{dJ}{d\theta}$ implique de multiplier des Jacobiennes √† chaque pas de temps :
$$\frac{ds_{t+1}}{d\theta} = \frac{df}{ds_t} \frac{ds_t}{d\theta} + \frac{df}{da_t} \frac{da_t}{d\theta}$$
* **Gradients Explosifs/Disparaissants :** Comme pour les RNNs, multiplier de nombreuses matrices Jacobiennes sur un long horizon $T$ rend l'optimisation num√©riquement instable.
* **Sensibilit√© aux Param√®tres :** Les m√©thodes de "Shooting" sont tr√®s sensibles aux petites erreurs de mod√®le qui s'amplifient exponentiellement.

### Solution : Collocation (Optimisation avec Contraintes)
Au lieu d'optimiser les actions s√©quentiellement (Shooting), on optimise tout la trajectoire $(s_1, a_1, \dots, s_T, a_T)$ simultan√©ment en traitant la dynamique $s_{t+1} = f(s_t, a_t)$ comme une **contrainte d'√©galit√©**. On utilise la m√©thode des Multiplicateurs de Lagrange (Dual Descent). C'est plus stable mais complexe √† impl√©menter.

---

## 2. Approches "Dyna" (Model-Based pour acc√©l√©rer Model-Free)

L'id√©e est d'utiliser le mod√®le appris comme un **simulateur** pour g√©n√©rer des donn√©es suppl√©mentaires et entra√Æner un algorithme Model-Free (ex: TRPO, SAC, DQN).

### Algorithme G√©n√©ral (Style Dyna-Q)
1.  Collecter des donn√©es r√©elles $\mathcal{D}$.
2.  Apprendre le mod√®le $f_\phi$ sur $\mathcal{D}$.
3.  **Boucle Model-Free :**
    * √âchantillonner un √©tat $s$ (depuis $\mathcal{D}$).
    * Simuler une action et une transition avec le mod√®le : $s' = f_\phi(s, \pi(s))$.
    * Ajouter $(s, a, r, s')$ au buffer d'entra√Ænement.
    * Mettre √† jour $\pi$ avec ces donn√©es synth√©tiques.

### Model-Based Policy Optimization (MBPO)
Une innovation cl√© pour que cela fonctionne avec le Deep RL :
* Ne pas g√©n√©rer de longues trajectoires avec le mod√®le (l'erreur s'accumule trop vite).
* G√©n√©rer des **rollouts tr√®s courts** ($k=1$ ou $k=2$) en partant d'√©tats **r√©els** √©chantillonn√©s dans le replay buffer.
* Cela permet d'avoir des donn√©es tr√®s vari√©es sans trop de biais de mod√®le.

---

## 3. Mod√®les Locaux et Guided Policy Search (GPS)

Il est tr√®s difficile d'apprendre un mod√®le global $f_\phi(s,a)$ pr√©cis partout. En revanche, il est facile d'apprendre des mod√®les **locaux lin√©aires** autour d'une trajectoire sp√©cifique.

### Dynamique Lin√©aire Locale
Autour d'une trajectoire $(s_t, a_t)$, on approxime la dynamique par :
$$s_{t+1} \approx \mathbf{A}_t s_t + \mathbf{B}_t a_t + c_t$$
On peut apprendre ces matrices $\mathbf{A}_t, \mathbf{B}_t$ par r√©gression lin√©aire simple sur quelques √©chantillons.

### Contr√¥le Optimal Local (iLQR)
Si la dynamique est lin√©aire et le co√ªt quadratique, on peut r√©soudre le contr√¥le optimal exactement et efficacement avec **LQR** (Linear Quadratic Regulator).
Si le mod√®le n'est pas lin√©aire, on utilise **iLQR** (iterative LQR) pour ajuster it√©rativement la trajectoire.

### Algorithme GPS (Guided Policy Search)
GPS combine l'efficacit√© du contr√¥le optimal (iLQR) avec la g√©n√©ralisation des r√©seaux de neurones. C'est un algorithme de "Distillation".

1.  **Optimisation de Trajectoire (L'enseignant) :** Utiliser iLQR avec des mod√®les locaux pour trouver des trajectoires optimales et des contr√¥leurs locaux simples pour diverses conditions initiales.
2.  **Apprentissage Supervis√© (L'√©l√®ve) :** Entra√Æner une politique neuronale globale $\pi_\theta$ pour imiter les actions des contr√¥leurs locaux sur ces trajectoires.
    $$\min_\theta \sum_{t} D_{KL}(\pi_{\text{local}}(a_t|s_t) || \pi_\theta(a_t|s_t))$$
3.  **Adaptation :** La politique globale permet de g√©n√©raliser √† de nouveaux √©tats, et sert √† guider la collecte de nouvelles donn√©es pour raffiner les mod√®les locaux.

---

## üîë R√©sum√© des M√©thodes

| M√©thode | Principe | Avantages | Inconv√©nients |
| :--- | :--- | :--- | :--- |
| **Backprop through time** | Calculer $\nabla_\theta J$ via la cha√Æne de d√©riv√©es du mod√®le. | Conceptuellement simple. | Gradients instables (Vanishing/Exploding), tr√®s sensible au biais du mod√®le. |
| **Dyna / MBPO** | Utiliser le mod√®le pour g√©n√©rer des donn√©es d'entra√Ænement pour un algo Model-Free. | Tr√®s efficace (Sample efficient), flexible. | N√©cessite un calibrage fin de l'horizon de g√©n√©ration pour √©viter le biais. |
| **Guided Policy Search** | Utiliser iLQR (mod√®les locaux) pour guider une politique globale. | Tr√®s stable, efficace pour la robotique complexe. | Complexe √† impl√©menter, repose sur la lin√©arisation locale. |

---
*Source: CS 285 Lecture 11 Slides, Instructor: Sergey Levine, UC Berkeley.*