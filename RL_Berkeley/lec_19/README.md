# CS 285 : Reframing Control as an Inference Problem (Lecture 19)

Ce document r√©sume le cours sur la reformulation du contr√¥le et du RL comme un probl√®me d'**inf√©rence probabiliste**.
Au lieu de maximiser simplement une somme de r√©compenses, on mod√©lise la t√¢che comme la g√©n√©ration d'une trajectoire conditionn√©e par l'observation d'une variable d'optimalit√©. Cela conduit naturellement √† l'**entropie maximale** (Maximum Entropy RL) et √† des politiques stochastiques robustes ("Soft Optimality").

## üß† L'Id√©e Fondamentale

Dans le RL classique, on cherche la politique optimale $\pi^*$ :
$$\pi^* = \arg\max_\pi \sum_t E[r(s_t, a_t)]$$

Dans l'approche **Inf√©rence**, on introduit une variable binaire $\mathcal{O}_t$ (Optimalit√©) qui vaut 1 si l'agent est optimal au temps $t$. La probabilit√© d'√™tre optimal est d√©finie par la r√©compense :
$$p(\mathcal{O}_t | s_t, a_t) = \exp(r(s_t, a_t))$$

Le probl√®me de RL devient alors : **Calculer la distribution a posteriori des trajectoires sachant qu'on est optimal tout le temps.**
$$p(\tau | \mathcal{O}_{1:T})$$

---

## üìâ Inf√©rence Exacte et le Probl√®me de l'Optimisme

Si on applique les algorithmes d'inf√©rence classiques (type HMM Forward-Backward) √† ce mod√®le graphique :
1.  **Messages Backward ($\beta_t$) :** Correspondent √† la "Valeur" ("Reward-to-go").
    $$\beta_t(s_t) \approx \exp(V(s_t))$$
2.  **La Politique :**
    $$p(a_t | s_t, \mathcal{O}_{1:T}) \propto \exp(Q(s_t, a_t) - V(s_t))$$

### Le Probl√®me (Optimisme)
Si on fait de l'inf√©rence na√Øve pour trouver la trajectoire la plus probable, le mod√®le va "tricher". Il va supposer que la dynamique $p(s_{t+1}|s_t, a_t)$ va *aussi* changer pour nous aider √† atteindre le but (ex: "J'ai gagn√© au loto, donc la probabilit√© de gagner devait √™tre de 100%").
Math√©matiquement : $p(s_{t+1} | s_t, a_t, \mathcal{O}_{1:T}) \neq p_{env}(s_{t+1} | s_t, a_t)$.

---

## üõ†Ô∏è Inf√©rence Variationnelle (Variational Inference)

Pour r√©soudre ce probl√®me, on fixe la dynamique (elle doit rester celle de l'environnement) et on cherche une distribution de trajectoire $q(\tau)$ qui soit proche du posterior optimal $p(\tau|\mathcal{O})$ tout en respectant la physique.

On minimise la divergence KL :
$$J(q) = D_{KL}(q(\tau) || p(\tau|\mathcal{O}_{1:T}))$$

Cela revient √† maximiser la **Borne Inf√©rieure Variationnelle (ELBO)** :
$$\sum_t E_{(s_t, a_t) \sim q} [r(s_t, a_t) + \mathcal{H}(q(a_t | s_t))]$$

**R√©sultat Cl√© :** Le RL probabiliste est √©quivalent √† maximiser la r√©compense **PLUS** l'entropie de la politique ($\mathcal{H}$). C'est le fondement du **Maximum Entropy RL**.

---

## ü§ñ Algorithmes "Soft"

Les √©quations de Bellman changent pour inclure cette "douceur" (Softness) due √† l'entropie. Le `max` dur est remplac√© par un `softmax` (LogSumExp).

### 1. Soft Value Iteration
Au lieu de $V(s) = \max_a Q(s,a)$, on a :
$$V_{soft}(s) = \log \int \exp(Q(s, a)) da \approx \text{soft\_max}_a Q(s,a)$$

### 2. Soft Q-Learning
L'algorithme modifie la cible de l'apprentissage Q (Target) :
$$y_i = r_i + \gamma V_{soft}(s_i') = r_i + \gamma \log \sum_{a'} \exp(Q_\phi(s_i', a'))$$
La politique induite est stochastique :
$$\pi(a|s) = \exp(Q_\phi(s, a) - V_{soft}(s))$$

### 3. Soft Actor-Critic (SAC)
C'est l'algorithme pratique le plus courant d√©riv√© de cette th√©orie.
1.  **Critic :** Apprend $Q(s,a)$ en minimisant l'erreur de Bellman douce.
2.  **Actor :** Apprend une politique $\pi_\theta(a|s)$ pour minimiser la divergence KL avec la distribution exponentielle de Q (Projection d'information).
    $$J(\pi) = D_{KL} \left( \pi(\cdot|s) \Big|\Big| \frac{\exp(Q(s, \cdot))}{Z} \right)$$

---

## ‚úÖ Pourquoi faire du Soft RL ? (Avantages)

1.  **Exploration :** L'agent cherche √† maximiser l'entropie, ce qui l'incite naturellement √† explorer des actions diverses et √† ne pas converger pr√©matur√©ment vers une solution sous-optimale d√©terministe.
2.  **Robustesse :** La politique apprise est plus "large" (couvre plus d'√©tats) et r√©siste mieux aux perturbations que les politiques "bang-bang" (tout ou rien) du RL standard.
3.  **Multimodalit√© :** Si deux actions sont aussi bonnes, Soft RL apprendra √† jouer les deux avec probabilit√© √©gale (alors que Q-learning en choisirait une arbitrairement).
4.  **Pretraining & Transfert :** Les politiques √† haute entropie sont d'excellents points de d√©part pour le finetuning sur des t√¢ches plus sp√©cifiques.

---

## üîë R√©sum√© Math√©matique

| Concept | Standard RL (Hard) | Inference RL (Soft) |
| :--- | :--- | :--- |
| **Objectif** | $\sum r_t$ | $\sum r_t + \alpha \mathcal{H}(\pi)$ |
| **Value Function** | $V(s) = \max_a Q(s,a)$ | $V(s) = \log \int \exp Q(s,a) da$ |
| **Politique** | D√©terministe (Greedy) | Stochastique (Boltzmann/Energy-based) |
| ** Bellman Backup** | $r + \gamma \max Q'$ | $r + \gamma \text{softmax} Q'$ |

---
[cite_start]*Source: CS 285 Lecture 19 Slides[cite: 434, 435, 436], Instructor: Sergey Levine, UC Berkeley.*