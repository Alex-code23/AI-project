# CS 285 : Policy Gradients (Lecture 5)

Ce document rÃ©sume le cours sur les **Policy Gradients** (Gradients de Politique). L'objectif principal est d'apprendre une politique paramÃ©trÃ©e $\pi_\theta(a|s)$ qui maximise la somme des rÃ©compenses espÃ©rÃ©es en optimisant directement les paramÃ¨tres $\theta$ par descente de gradient.

## ğŸ¯ Objectif du Reinforcement Learning

Le but est de maximiser l'espÃ©rance des rÃ©compenses cumulÃ©es sur une trajectoire $\tau$ :

$$\theta^* = \arg \max_\theta J(\theta)$$
$$J(\theta) = E_{\tau \sim p_\theta(\tau)} \left[ \sum_t r(s_t, a_t) \right]$$

[cite_start]OÃ¹ la probabilitÃ© d'une trajectoire $p_\theta(\tau)$ dÃ©pend de la politique et de la dynamique du systÃ¨me (bien que la dynamique s'annule dans le gradient final)[cite: 36, 54].

---

## ğŸ§® DÃ©rivation du Gradient (The Log-Derivative Trick)

Pour calculer le gradient de l'espÃ©rance $\nabla_\theta J(\theta)$, on utilise l'identitÃ© $\nabla p(\tau) = p(\tau) \nabla \log p(\tau)$ :

$$\nabla_\theta J(\theta) = E_{\tau \sim p_\theta(\tau)} [\nabla_\theta \log p_\theta(\tau) r(\tau)]$$

En simplifiant grÃ¢ce Ã  la propriÃ©tÃ© de Markov (la dynamique $p(s_{t+1}|s_t, a_t)$ ne dÃ©pend pas de $\theta$), on obtient la formule standard :

$$\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \left( \sum_{t=1}^T \nabla_\theta \log \pi_\theta(a_{i,t}|s_{i,t}) \right) \left( \sum_{t=1}^T r(s_{i,t}, a_{i,t}) \right)$$

**Intuition :** Cette formule formalise l'essai-erreur ("trial and error"). [cite_start]Elle augmente la probabilitÃ© des trajectoires ayant une rÃ©compense Ã©levÃ©e et diminue celle des trajectoires Ã  faible rÃ©compense[cite: 121, 122].

---

## ğŸ¤– Algorithme REINFORCE

[cite_start]L'algorithme de base fonctionne comme suit[cite: 61]:

1.  **Ã‰chantillonner** $\{\tau^i\}$ Ã  partir de la politique $\pi_\theta(a_t|s_t)$ (exÃ©cuter la politique sur le robot/environnement).
2.  **Estimer le gradient** $\nabla_\theta J(\theta)$ en utilisant les Ã©chantillons.
3.  **Mettre Ã  jour les paramÃ¨tres** : $\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)$.

---

## ğŸ“‰ RÃ©duction de la Variance

[cite_start]Le gradient de politique brut a une **variance trÃ¨s Ã©levÃ©e**, ce qui rend l'apprentissage instable[cite: 156]. Deux techniques principales sont utilisÃ©es pour la rÃ©duire :

### 1. CausalitÃ© (Reward-to-go)
La politique au temps $t$ ne peut pas affecter les rÃ©compenses passÃ©es ($t' < t$). On remplace la somme totale des rÃ©compenses par la somme des rÃ©compenses futures (Reward-to-go $\hat{Q}_{i,t}$) :

$$\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \nabla_\theta \log \pi_\theta(a_{i,t}|s_{i,t}) \underbrace{\sum_{t'=t}^T r(s_{i,t'}, a_{i,t'})}_{\hat{Q}_{i,t}}$$

[cite_start]Cette modification est valide car le futur n'affecte pas le passÃ©[cite: 176, 177].

### 2. Baselines (Lignes de base)
On peut soustraire une valeur constante ou dÃ©pendante de l'Ã©tat (baseline $b$) Ã  la rÃ©compense sans biaiser le gradient (car $E[\nabla \log p(\tau) \cdot b] = 0$) :

$$\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \nabla_\theta \log p_\theta(\tau) [r(\tau) - b]$$

* **Pourquoi ?** Cela centre les retours. Si toutes les rÃ©compenses sont positives, sans baseline, on ne ferait qu'augmenter les probabilitÃ©s de tout, juste Ã  des vitesses diffÃ©rentes.
* [cite_start]**Baseline optimale :** La rÃ©compense moyenne pondÃ©rÃ©e par la magnitude du gradient[cite: 195].

---

## ğŸ”„ Off-Policy Policy Gradients (Importance Sampling)

L'apprentissage "On-policy" est inefficace car chaque Ã©chantillon n'est utilisÃ© qu'une fois. [cite_start]Pour utiliser des Ã©chantillons d'une ancienne politique $\bar{\pi}$, on utilise l'**Importance Sampling (IS)**[cite: 220, 225]:

$$J(\theta') = E_{\tau \sim \pi_\theta(\tau)} \left[ \frac{\pi_{\theta'}(\tau)}{\pi_\theta(\tau)} r(\tau) \right]$$

[cite_start]Cela permet de rÃ©utiliser les donnÃ©es passÃ©es, mais introduit un produit de ratios qui peut mener Ã  une variance exponentielle en fonction de l'horizon $T$[cite: 252].

---

## ğŸ’» ImplÃ©mentation avec DiffÃ©rentiation Automatique

En pratique, on n'implÃ©mente pas la formule du gradient directement. [cite_start]On dÃ©finit une "pseudo-loss" que l'on minimise avec un optimiseur standard (comme Adam)[cite: 270, 294]:

$$\tilde{J}(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \log \pi_\theta(a_{i,t}|s_{i,t}) \cdot \hat{Q}_{i,t}$$

En TensorFlow/PyTorch : `loss = reduce_mean(cross_entropy * q_values)`.

---

## ğŸš€ Sujets AvancÃ©s : Natural Policy Gradient

Le gradient standard suit la direction la plus raide dans l'espace des paramÃ¨tres (Euclidien), mais une petite modification de paramÃ¨tres peut changer drastiquement la politique (probabilitÃ©s).

[cite_start]**Solution :** Limiter le changement de la distribution de la politique (Divergence KL) plutÃ´t que le changement des paramÃ¨tres[cite: 352].

$$\theta \leftarrow \theta + \alpha F^{-1} \nabla_\theta J(\theta)$$

OÃ¹ $F$ est la **matrice d'information de Fisher**. [cite_start]Cela mÃ¨ne Ã  des algorithmes comme **TRPO** (Trust Region Policy Optimization) et **PPO**[cite: 367, 374].

---

## âœ… Avantages et âŒ InconvÃ©nients

| Avantages | InconvÃ©nients |
| :--- | :--- |
| **Direct :** Optimise directement l'objectif de RL. | [cite_start]**Haute Variance :** Le gradient est trÃ¨s bruitÃ©, nÃ©cessite de gros batchs[cite: 298]. |
| **Continu :** GÃ¨re facilement les espaces d'actions continus (ex: robots). | [cite_start]**EfficacitÃ© :** Souvent "On-policy", donc nÃ©cessite beaucoup d'Ã©chantillons (sample inefficient)[cite: 217]. |
| **Convergence :** Garanties de convergence locale. | **Optima Locaux :** Peut rester coincÃ© dans des optimums locaux. |
| [cite_start]**Partiellement Observable :** Fonctionne sans modification si l'Ã©tat n'est pas complet ($o_t$ vs $s_t$)[cite: 149]. | [cite_start]**Sensible :** Difficile Ã  rÃ©gler (learning rates instables sans mÃ©thodes avancÃ©es comme Adam ou Natural Gradient)[cite: 302]. |

---
*Source: CS 285 Lecture 5 Slides, Instructor: Sergey Levine, UC Berkeley.*