# CS 285 : Value Function Methods (Lecture 7)

Ce document r√©sume le cours sur les **Value Function Methods** (M√©thodes bas√©es sur la fonction de valeur). Contrairement aux m√©thodes pr√©c√©dentes qui optimisent directement une politique, ces algorithmes apprennent une fonction de valeur ($V$ ou $Q$) et d√©finissent la politique comme √©tant celle qui maximise cette valeur (politique gloutonne/greedy).

## üéØ Objectif : Omettre le Policy Gradient

L'objectif est de trouver la politique optimale sans la repr√©senter explicitement par un r√©seau de neurones param√©tr√© $\pi_\theta$. On apprend plut√¥t une fonction de valeur neuronale $V_\phi(s)$ ou $Q_\phi(s,a)$.

La politique devient implicite (Argmax policy) :
$$\pi(a_t|s_t) = \begin{cases} 1 & \text{si } a_t = \arg\max_a A^\pi(s_t, a) \\ 0 & \text{sinon} \end{cases}$$
O√π $A^\pi(s,a)$ est la fonction d'avantage.

---

## üîÑ De l'It√©ration de Politique √† l'It√©ration de Valeur

### 1. Policy Iteration (It√©ration de Politique)
L'algorithme alterne entre deux √©tapes jusqu'√† convergence :
1.  **Policy Evaluation :** Calculer $A^\pi(s,a)$ pour la politique actuelle (souvent co√ªteux).
2.  **Policy Improvement :** Mettre √† jour la politique $\pi \leftarrow \arg\max A^\pi$.

### 2. Value Iteration (It√©ration de Valeur)
On simplifie le processus en combinant les deux √©tapes. On met √† jour directement la fonction de valeur optimale $V^*$ sans passer par une politique interm√©diaire :

$$V(s) \leftarrow \max_a \sum_{s'} p(s'|s,a) [r(s,a) + \gamma V(s')]$$

---

## üß† Fitted Value Iteration & Q-Iteration

Pour les espaces d'√©tats continus ou tr√®s grands, on ne peut pas utiliser de tableaux. On utilise un approximateur de fonction (R√©seau de Neurones) avec param√®tres $\phi$.

### Fitted Value Iteration
On apprend $V_\phi(s)$ en minimisant l'erreur quadratique par rapport √† une cible $y_i$ :
$$y_i = \max_{a_i} (r(s_i, a_i) + \gamma E[V_\phi(s'_i)])$$
$$\mathcal{L}(\phi) = \frac{1}{2} \sum_i || V_\phi(s_i) - y_i ||^2$$
* **Limitation :** Pour calculer le $\max_a$ et l'esp√©rance $E$, il faut conna√Ætre la dynamique $p(s'|s,a)$ (le mod√®le de transition).

### Fitted Q-Iteration (FQI)
Pour se passer de mod√®le (Model-Free), on apprend la fonction $Q_\phi(s,a)$.

**Algorithme complet :**
1.  **Collecte de donn√©es :** Obtenir un dataset $\mathcal{D} = \{(s_i, a_i, s'_i, r_i)\}$ en utilisant une politique d'exploration.
2.  **Calcul des cibles :** $y_i = r_i + \gamma \max_{a'} Q_\phi(s'_i, a')$.
3.  **R√©gression (Update) :** Entra√Æner $\phi$ pour minimiser $\sum (Q_\phi(s_i, a_i) - y_i)^2$.
4.  **It√©ration :** R√©p√©ter les √©tapes 2 et 3 $K$ fois.

C'est un algorithme **Off-Policy** : on peut utiliser des donn√©es collect√©es par n'importe quelle politique pass√©e.

---

## üìâ Th√©orie et Convergence

Pourquoi ces m√©thodes fonctionnent-elles (ou √©chouent-elles) ?

### Cas Tabulaire (Tableau)
L'op√©rateur de Bellman $\mathcal{B}$ est une **contraction** pour la norme $\infty$ (max norm).
$$|| \mathcal{B}V - \mathcal{B}\bar{V} ||_\infty \le \gamma || V - \bar{V} ||_\infty$$
Cela garantit que *Value Iteration* converge toujours vers la solution unique $V^*$.

### Cas "Fitted" (R√©seaux de Neurones)
L'algorithme alterne entre l'op√©rateur de Bellman $\mathcal{B}$ et une √©tape de projection $\Pi$ (la r√©gression/minimisation de l'erreur).
* $\Pi$ est une contraction pour la norme $L_2$ (Euclidienne).
* $\mathcal{B}$ est une contraction pour la norme $L_\infty$.
* **Probl√®me :** La composition $\Pi \mathcal{B}$ n'est **pas** une contraction.
* **Cons√©quence :** Fitted Q-Iteration n'est **pas garanti de converger** et peut osciller ou diverger avec des r√©seaux de neurones.

---

## üîç Exploration

Puisque la politique d√©riv√©e est d√©terministe ($a = \arg\max Q$), l'exploration explicite est cruciale.
* **Epsilon-Greedy :** Avec probabilit√© $\epsilon$, choisir une action au hasard ; sinon, choisir l'action optimale.
* **Boltzmann Exploration :** Choisir les actions proportionnellement √† $\exp(Q(s,a))$.

---

## ‚úÖ Avantages et ‚ùå Inconv√©nients

| Avantages | Inconv√©nients |
| :--- | :--- |
| **Sample Efficiency :** M√©thodes **Off-policy**. Tr√®s efficaces car elles r√©utilisent les donn√©es pass√©es. | **Convergence :** Aucune garantie th√©orique de convergence avec les r√©seaux de neurones. Risque de divergence. |
| **Simplicit√© :** Pas de gradient de politique √† haute variance. | **Optimisation :** Calculer $\max_a Q(s,a)$ est facile pour les actions discr√®tes mais difficile pour les actions continues. |
| **Vitesse :** La r√©gression supervis√©e (√©tape 3) est souvent plus stable que la mont√©e de gradient sur une politique. | **Qualit√© :** La politique apprise peut √™tre biais√©e par les erreurs d'approximation de la fonction $Q$. |

---
*Source: CS 285 Lecture 7 Slides, Instructor: Sergey Levine, UC Berkeley.*