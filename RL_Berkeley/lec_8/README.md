# CS 285 : Deep RL with Q-Functions (Lecture 8)

Ce document r√©sume le cours sur l'application des r√©seaux de neurones profonds au Q-Learning. Il explique pourquoi le Q-Learning "na√Øf" √©choue avec les r√©seaux de neurones et introduit les algorithmes classiques comme **DQN** et ses am√©liorations pour les actions discr√®tes et continues.

## ‚ö†Ô∏è Les probl√®mes du Q-Learning avec R√©seaux de Neurones

L'algorithme "online Q-learning" standard ressemble √† une descente de gradient, mais il n'en est pas une vraie. Lorsqu'on utilise des approximateurs de fonction non-lin√©aires (r√©seaux de neurones), deux probl√®mes majeurs causent l'instabilit√© ou la divergence:

### 1. √âchantillons Corr√©l√©s (Correlated Samples)
Dans l'apprentissage en ligne, les donn√©es arrivent s√©quentiellement $(s_t, a_t, r_t, s_{t+1})$. Ces √©chantillons sont fortement corr√©l√©s temporellement. La descente de gradient stochastique (SGD) suppose que les donn√©es sont i.i.d. (ind√©pendantes et identiquement distribu√©es). Sans cela, le r√©seau sur-apprend sur les donn√©es r√©centes et oublie les anciennes.

### 2. Cibles Mouvantes (Moving Targets)
La cible de la r√©gression $y_i = r + \gamma \max_{a'} Q_\phi(s', a')$ d√©pend des m√™mes param√®tres $\phi$ que ceux que l'on est en train d'optimiser.
Contrairement √† la r√©gression supervis√©e o√π la cible est fixe, ici la cible bouge √† chaque mise √† jour. Cela cr√©e des boucles de r√©troaction instables et des oscillations.

---

## üéÆ La Solution : DQN (Deep Q-Network)

L'algorithme DQN (Mnih et al., 2013/2015) introduit deux m√©canismes pour stabiliser l'apprentissage sur les jeux Atari :

### 1. Replay Buffer (Tampon de R√©p√©tition)
Au lieu d'apprendre sur la derni√®re transition, on stocke les transitions $(s, a, r, s')$ dans un grand buffer $\mathcal{B}$. On √©chantillonne ensuite un **batch al√©atoire** pour la mise √† jour.
* **Avantage :** Brise la corr√©lation temporelle et rend les √©chantillons plus proches de l'i.i.d..

### 2. Target Network (R√©seau Cible)
On utilise un second r√©seau $Q_{\phi'}$ (Target Network) pour calculer la cible, dont les param√®tres $\phi'$ sont une copie retard√©e de $\phi$ (mise √† jour p√©riodique ou moyenne exponentielle/Polyak averaging).
* **Cible :** $y = r + \gamma \max_{a'} Q_{\phi'}(s', a')$
* **Avantage :** La cible reste stable pendant un certain temps, transformant le probl√®me en une s√©rie de probl√®mes de r√©gression supervis√©e plus stables.

---

## üìà Am√©liorations de DQN

### Double Q-Learning
Le Q-Learning standard surestime syst√©matiquement les valeurs Q car $E[\max(X)] \ge \max(E[X])$ (le bruit positif est amplifi√© par le max).
**Solution :** D√©coupler la *s√©lection* de l'action et son *√©valuation*.
* Utiliser le r√©seau actuel $\phi$ pour choisir l'action.
* Utiliser le r√©seau cible $\phi'$ pour √©valuer sa valeur.
$$y = r + \gamma Q_{\phi'}(s', \arg\max_{a'} Q_\phi(s', a'))$$.

### Multi-step Returns (N-step)
Au lieu d'utiliser un seul pas de r√©compense (Bellman pur), on utilise $N$ pas avant de bootstraper.
$$y_{i,t} = \sum_{k=0}^{N-1} \gamma^k r_{t+k} + \gamma^N \max_{a'} Q_{\phi'}(s_{i, t+N}, a')$$.
* **Trade-off :** R√©duit le biais (moins de d√©pendance √† l'estimation Q initiale) mais augmente la variance (plus de r√©compenses stochastiques accumul√©es). Souvent, $N$ entre 3 et 5 fonctionne bien.

---

## ü§ñ Q-Learning pour Actions Continues

L'op√©ration $\max_a Q(s,a)$ est difficile quand l'espace d'action est continu.

### 1. Optimisation Stochastique
Utiliser des m√©thodes comme CEM ou CMA-ES pour trouver le max, ou une simple descente de gradient sur l'input $a$. C'est souvent trop lent.

### 2. Normalized Advantage Functions (NAF)
On force l'architecture du r√©seau Q √† √™tre quadratique par rapport √† l'action $a$, ce qui rend le maximum analytique et facile √† calculer ($argmax$ est $\mu(s)$).
$$Q(s,a) = -\frac{1}{2}(a - \mu(s))^T P(s) (a - \mu(s)) + V(s)$$

### 3. DDPG (Deep Deterministic Policy Gradient)
On apprend un r√©seau "acteur" $\mu_\theta(s)$ qui pr√©dit l'action maximisant $Q$.
* Le Critique apprend $Q_\phi(s,a)$ (similaire √† DQN).
* L'Acteur apprend $\theta$ pour maximiser $Q_\phi(s, \mu_\theta(s))$ via la r√®gle de la cha√Æne.
$$\frac{dQ}{d\theta} = \frac{dQ}{da} \frac{da}{d\theta}$$
C'est essentiellement du Q-Learning o√π le `max` est approxim√© par un r√©seau de neurones.

---

## üõ†Ô∏è Conseils Pratiques pour le Q-Learning

* **Fiabilit√© :** Le Q-Learning est moins stable que les Policy Gradients. Il n√©cessite beaucoup de r√©glages d'hyperparam√®tres.
* **Exploration :** Commence avec un $\epsilon$ √©lev√© et diminue-le lentement.
* **Stabilit√© :**
    * Utiliser **Double Q-Learning** (aide presque toujours).
    * Utiliser des taux d'apprentissage (learning rates) bas.
    * Clipper les gradients ou utiliser la **Huber Loss** (pour √©viter que les erreurs Bellman √©lev√©es ne d√©stabilisent tout).
* **Temps :** La convergence peut √™tre tr√®s longue, ne pas arr√™ter l'entra√Ænement trop t√¥t.

---

## ‚úÖ Avantages et ‚ùå Inconv√©nients

| Avantages | Inconv√©nients |
| :--- | :--- |
| **Sample Efficiency :** Tr√®s efficace en donn√©es gr√¢ce au Replay Buffer (Off-policy). | **Convergence :** Pas de garantie de convergence avec l'approximation de fonction non-lin√©aire. |
| **G√©n√©ralit√© :** Fonctionne bien sur des t√¢ches complexes (Atari, Robotique continue avec DDPG/SAC). | **Instabilit√© :** Tr√®s sensible aux hyperparam√®tres. |
| **Pas de politique explicite :** (Pour DQN) Simplifie l'architecture (un seul r√©seau). | **Actions Continues :** Plus complexe √† adapter (n√©cessite DDPG/NAF). |

---
*Source: CS 285 Lecture 8 Slides, Instructor: Sergey Levine, UC Berkeley.*