# CS 285 : Exploration Part 1 - Bandits & Count-Based Exploration (Lecture 13)

Ce document r√©sume la premi√®re partie du cours sur l'**Exploration**.
Dans les probl√®mes simples, l'exploration al√©atoire ($\epsilon$-greedy) suffit. Mais dans des environnements complexes avec des r√©compenses √©parses (ex: *Montezuma's Revenge*), l'agent peut ne jamais trouver la r√©compense par hasard. Ce cours introduit des m√©thodes pour une exploration **dirig√©e** et **intelligente**.

## ‚ö†Ô∏è Le Probl√®me : R√©compenses √âparses (Sparse Rewards)

Si la probabilit√© de trouver une r√©compense par hasard est exponentiellement faible par rapport √† la longueur de l'√©pisode (horizon), les m√©thodes classiques (Policy Gradient, Q-Learning avec $\epsilon$-greedy) √©chouent.

L'objectif est de remplacer l'exploration non dirig√©e (bruit al√©atoire sur les actions) par une exploration dirig√©e vers les zones inconnues de l'espace d'√©tats.

---

## üé∞ Intuition : Les Bandits Manchots (Multi-Armed Bandits)

Avant de passer au Deep RL, on regarde comment le probl√®me est r√©solu th√©oriquement dans le cas simple (1 √©tat, $N$ actions).

### 1. Optimism in the Face of Uncertainty (UCB)
On ne choisit pas l'action avec la meilleure moyenne empirique, mais celle avec la **borne sup√©rieure de confiance** la plus √©lev√©e.
$$a_t = \arg\max_a \left( \hat{\mu}(a) + \sqrt{\frac{2 \ln T}{N(a)}} \right)$$
* $\hat{\mu}(a)$ : R√©compense moyenne estim√©e (Exploitation).
* $N(a)$ : Nombre de fois que l'action a √©t√© choisie.
* Le terme racine est un **bonus d'exploration** qui diminue quand $N(a)$ augmente.

### 2. Thompson Sampling (Posterior Sampling)
On maintient une distribution de probabilit√© sur les r√©compenses possibles $p(\theta | \mathcal{D})$.
* On √©chantillonne un mod√®le $\hat{\theta} \sim p(\theta | \mathcal{D})$.
* On agit de fa√ßon optimale selon $\hat{\theta}$.
* Cela permet une exploration proportionnelle √† l'incertitude ("Probability Matching").

### 3. Information Gain
On choisit l'action qui maximise le gain d'information attendu sur la dynamique ou les r√©compenses (r√©duire l'entropie de notre croyance).

---

## üß† Deep RL : Count-Based Exploration

Dans un MDP (Markov Decision Process), l'analogue de l'UCB serait d'ajouter un **Bonus d'Exploration** intrins√®que √† la r√©compense :

$$r^+(s, a) = r_{\text{env}}(s, a) + \mathcal{B}(N(s))$$

O√π $\mathcal{B}(N(s))$ est souvent proportionnel √† $\frac{1}{\sqrt{N(s)}}$.

### Le D√©fi des Espaces Continus
Dans des environnements complexes (images, robotique), on ne revisite **jamais** exactement le m√™me √©tat (pixels l√©g√®rement diff√©rents). Donc $N(s)$ est toujours √©gal √† 0 ou 1, ce qui rend le comptage na√Øf inutile.

### Solution : Pseudo-Counts via Mod√®les de Densit√©
L'id√©e est d'utiliser un mod√®le g√©n√©ratif pour estimer la densit√© de probabilit√© $p_\theta(s)$ (la probabilit√© d'observer cet √©tat selon nos donn√©es pass√©es).
On peut relier la probabilit√© au comptage via :
$$N(s) \approx \frac{1}{\hat{p}(s)}$$
Si l'√©tat a une probabilit√© faible (surprenant/nouveau), son pseudo-compte est faible, donc le bonus est √©lev√©.

#### Algorithmes Concrets :
1.  **Mod√®les G√©n√©ratifs (CTS / PixelCNN) :** (Bellemare et al. 2016)
    * Entra√Æner un mod√®le pour pr√©dire la probabilit√© des pixels.
    * Utiliser la "probabilit√© d'enregistrement" (recording probability) pour d√©river un pseudo-compte $\hat{N}$.
    * Ajouter un bonus $\frac{1}{\sqrt{\hat{N}}}$ √† la r√©compense.

2.  **Hash-Based Counting (SimHash) :** (Tang et al. 2017)
    * Utiliser un Auto-Encodeur pour compresser l'image en un code latent $\phi(s)$.
    * Utiliser du **Locality-Sensitive Hashing (LSH)** pour discr√©tiser cet espace continu en "buckets" discrets.
    * Compter simplement les visites dans chaque bucket : $N(h(s))$.
    * C'est simple et tr√®s efficace.

3.  **Implicit Density (EX2) :** (Fu et al. 2017)
    * Entra√Æner un classifieur √† distinguer les √©tats visit√©s de bruits al√©atoires. La performance du classifieur donne une estimation de la densit√©.

---

## ‚öñÔ∏è R√©sum√© des Approches Count-Based

| M√©thode | Principe | Avantages | Inconv√©nients |
| :--- | :--- | :--- | :--- |
| **UCB (Tabulaire)** | Compter $N(s,a)$ dans un tableau. | Th√©oriquement optimal. | Impossible pour les grands espaces d'√©tats. |
| **Density Models** | Estimer $p(s)$ avec un r√©seau (PixelCNN/VAE) pour d√©river $\hat{N}$. | G√®re les images directement. | Les mod√®les g√©n√©ratifs sont lourds et difficiles √† entra√Æner. |
| **Hash-Based** | Discr√©tiser l'espace latent (SimHash) et compter. | Rapide, simple √† impl√©menter. | Perd de l'information (aliasing) √† cause du hachage. |

---

## üîë Points Cl√©s √† retenir
* L'exploration $\epsilon$-greedy est insuffisante pour les probl√®mes √† r√©compenses √©parses.
* Le principe d'**"Optimisme face √† l'incertitude"** sugg√®re d'ajouter un bonus aux √©tats peu visit√©s.
* En Deep RL, on ne peut pas compter les √©tats. On utilise des **Pseudo-Comptes** d√©riv√©s de la densit√© de probabilit√© ($p(s)$) ou de la discr√©tisation (Hashing).
* L'objectif final est de modifier la fonction de r√©compense : $r_{total} = r_{externe} + \alpha \cdot r_{exploration}$.

---
*Source: CS 285 Lecture 13 Slides, Instructor: Sergey Levine, UC Berkeley.*