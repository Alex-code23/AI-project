# CS 285 : Advanced Policy Gradients (Lecture 9)

Ce document r√©sume le cours sur les **Advanced Policy Gradients**. Alors que le Policy Gradient standard (REINFORCE) est instable et sensible au pas d'apprentissage (step size), ce cours introduit des m√©thodes pour garantir une am√©lioration monotone de la politique et stabiliser l'apprentissage en utilisant la g√©om√©trie de l'espace des distributions (Gradient Naturel, TRPO).

## üéØ Motivation : Policy Gradient comme Policy Iteration

L'objectif est de voir le Policy Gradient non plus comme une simple mont√©e de gradient stochastique, mais comme une approximation d'une **Policy Iteration**.

On cherche une mise √† jour $\pi'$ telle que $J(\pi') \ge J(\pi)$.
Pour cela, on utilise l'identit√© de l'avantage :
$$J(\pi') = J(\pi) + E_{\tau \sim \pi'} \left[ \sum_t \gamma^t A^\pi(s_t, a_t) \right]$$

Pour garantir une am√©lioration, il faut maximiser le second terme. Cependant, l'esp√©rance d√©pend de $\pi'$ (la nouvelle politique) qu'on ne conna√Æt pas encore.

---

## üöß Le Probl√®me du "Distribution Mismatch"

Si $\pi'$ est proche de $\pi$, on peut approximer l'esp√©rance sur $\pi'$ par une esp√©rance sur $\pi$ (en ignorant le changement de distribution d'√©tats) :

$$L_\pi(\pi') = J(\pi) + E_{s \sim \pi, a \sim \pi'} [ A^\pi(s, a) ]$$

Cependant, cette approximation introduit une erreur. La th√©orie (Schulman et al., TRPO) fournit une borne sur cette erreur en fonction de la divergence KL entre les politiques :

$$J(\pi') \ge L_\pi(\pi') - C \cdot \max_s D_{KL}(\pi(a|s) || \pi'(a|s))$$

* **Id√©e cl√© :** Si on maximise $L_\pi(\pi')$ tout en gardant la divergence KL petite (Trust Region), on garantit d'am√©liorer la vraie performance $J(\pi')$.

---

## üß¨ Natural Policy Gradient (NPG)

La mont√©e de gradient standard (Vanilla Gradient Ascent) suit la direction la plus raide dans l'espace des **param√®tres** (Euclidien). Or, une petite variation des param√®tres $\theta$ peut entra√Æner un changement √©norme de la distribution $\pi_\theta$ (la politique).

### 1. L'Objectif Contraint
On veut maximiser l'objectif sous une contrainte de changement de distribution :
$$\max_{\theta'} \nabla_\theta J(\theta)^T (\theta' - \theta)$$
$$\text{s.t. } D_{KL}(\pi_{\theta'} || \pi_\theta) \le \epsilon$$

### 2. Approximation Quadratique du KL
La divergence KL peut √™tre approxim√©e localement par la **Matrice d'Information de Fisher** ($F$) :
$$D_{KL}(\pi_{\theta'} || \pi_\theta) \approx \frac{1}{2} (\theta' - \theta)^T F (\theta' - \theta)$$
$$F = E_{\pi_\theta} [\nabla_\theta \log \pi_\theta(a|s) \nabla_\theta \log \pi_\theta(a|s)^T]$$

### 3. La Mise √† jour (Natural Gradient Update)
La solution analytique de ce probl√®me d'optimisation contraint donne la direction du gradient naturel :
$$\theta \leftarrow \theta + \alpha F^{-1} \nabla_\theta J(\theta)$$

Le pas $\alpha$ est choisi pour satisfaire la contrainte KL.

---

## üöÄ Algorithmes Pratiques

### TRPO (Trust Region Policy Optimization)
TRPO est une approximation pratique du NPG.
* Il utilise l'objectif "surrogate" (Importance Sampling) pour estimer $L_\pi(\pi')$.
* Il r√©sout $F^{-1} g$ efficacement en utilisant la m√©thode du **Gradient Conjugu√©** (Conjugate Gradient) pour √©viter d'inverser explicitement la matrice Hessienne/Fisher (tr√®s co√ªteux).
* Il impose une "Hard Constraint" sur le KL (Trust Region) via une recherche lin√©aire (Line Search).

### PPO (Proximal Policy Optimization)
Mentionn√© comme une simplification de TRPO.
* Au lieu d'une contrainte dure (Hard Constraint) difficile √† optimiser, PPO utilise une **r√©gularisation** (Clipping ou p√©nalit√© KL) directement dans la fonction objective.
* Beaucoup plus simple √† impl√©menter (gradient descent standard de premier ordre).

---

## ‚úÖ Avantages et ‚ùå Inconv√©nients

| Avantages | Inconv√©nients |
| :--- | :--- |
| **Stabilit√© :** Garantit une am√©lioration monotone (th√©oriquement) et √©vite les effondrements de performance ("policy collapse"). | **Complexit√© (TRPO/NPG) :** N√©cessite le calcul (ou l'approximation) de la matrice de Fisher et l'algorithme du Gradient Conjugu√©. |
| **Ind√©pendance de Param√©trage :** Le comportement de l'apprentissage d√©pend de la distribution, pas du choix arbitraire des param√®tres du r√©seau. | **Co√ªt de calcul :** Plus lourd qu'un simple gradient (REINFORCE/Adam). |
| **Pas d'apprentissage (Step Size) :** Plus robuste au choix du learning rate gr√¢ce √† la r√©gion de confiance adaptive. | **Impl√©mentation :** TRPO est notoirement difficile √† impl√©menter correctement par rapport √† PPO ou SAC. |

---
*Source: CS 285 Lecture 9 Slides, Instructor: Sergey Levine, UC Berkeley.*