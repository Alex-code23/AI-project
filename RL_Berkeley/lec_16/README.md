# CS 285 : Offline Reinforcement Learning Part 2 (Lecture 16)

Ce document r√©sume la deuxi√®me partie du cours sur l'**Offline RL**.
Apr√®s avoir couvert les m√©thodes Model-Free (contraintes de politique, CQL) dans le cours pr√©c√©dent, ce cours se concentre sur deux avanc√©es majeures :
1.  **Model-Based Offline RL :** Utiliser des mod√®les de dynamique pour mieux g√©n√©raliser, en g√©rant l'incertitude.
2.  **Sequence Modeling (Transformers) :** Traiter le RL comme un probl√®me de pr√©diction de s√©quence supervis√© √† grande √©chelle.

---

## üèóÔ∏è 1. Model-Based Offline RL

Les m√©thodes Model-Free (comme CQL) sont tr√®s stables mais restent parfois "coll√©es" aux donn√©es d'entra√Ænement. Les m√©thodes Model-Based ont le potentiel de mieux g√©n√©raliser en apprenant la physique du monde, mais elles souffrent du m√™me probl√®me de **d√©calage de distribution** : le mod√®le hallucine des transitions optimistes pour les actions hors distribution (OOD).

### Le Probl√®me : L'Exploitation du Mod√®le
Si on apprend un mod√®le $T_\phi(s'|s,a)$ sur le dataset statique et qu'on planifie avec, l'agent va chercher les actions o√π le mod√®le pr√©dit (√† tort) des √©tats futurs tr√®s avantageux.
* **Erreur du mod√®le :** L'erreur est faible sur les donn√©es $\mathcal{D}$, mais √©lev√©e ailleurs.
* **Cons√©quence :** La politique apprise diverge vers des zones o√π le mod√®le est faux.

### La Solution : P√©nalit√© d'Incertitude (MOPO / MOREL)
Pour emp√™cher l'agent d'aller l√† o√π le mod√®le n'est pas fiable, on modifie la fonction de r√©compense dans le processus de planification (ou d'apprentissage de politique).

1.  **Ensemble de Mod√®les :** Entra√Æner un ensemble de $N$ mod√®les dynamiques $\{T_{\theta_1}, \dots, T_{\theta_N}\}$ pour estimer l'incertitude √©pist√©mique (variance des pr√©dictions).
    $$u(s, a) = \text{Var}(s' | s, a)$$
2.  **MDP P√©nalis√© :** On construit un MDP artificiel o√π la r√©compense est p√©nalis√©e par cette incertitude :
    $$\tilde{r}(s, a) = r(s, a) - \lambda \cdot u(s, a)$$
3.  **Optimisation :** On apprend une politique (ou on planifie) pour maximiser cette r√©compense p√©nalis√©e.

**R√©sultat :** L'agent est "pessimiste". Il pr√©f√®re une action avec une r√©compense moyenne mais certaine, plut√¥t qu'une action avec une r√©compense potentiellement √©norme mais incertaine.

---

## üìú 2. RL as Sequence Modeling (Transformers)

Au lieu d'utiliser la programmation dynamique (Bellman/Q-Learning), peut-on traiter le RL comme un simple probl√®me de **pr√©diction de s√©quence** (comme GPT pour le langage) ?

### Decision Transformer (DT)
L'id√©e est de mod√©liser la trajectoire comme une s√©quence de tokens :
$$\tau = (\dots, R_t, s_t, a_t, R_{t+1}, s_{t+1}, a_{t+1}, \dots)$$
O√π $R_t$ est le **Return-to-go** (la somme des r√©compenses futures esp√©r√©es).

* **Entra√Ænement :** On entra√Æne un Transformer (GPT) de mani√®re supervis√©e pour pr√©dire le prochain token (surtout l'action $a_t$).
  $$a_t \sim P(a_t | R_t, s_t, a_{t-1}, \dots)$$
* **Inf√©rence (Test) :** On donne √† l'agent l'√©tat actuel $s_t$ et on lui "commande" un retour √©lev√© (ex: $R_{target} = \text{Max Score}$). Le mod√®le pr√©dit l'action qui est la plus probable pour obtenir ce retour, bas√© sur les statistiques du dataset.

### Trajectory Transformer (TT)
Similaire au DT, mais discretise les √©tats et les actions pour utiliser un mod√®le de langage standard. Il utilise la "Beam Search" pour planifier des trajectoires enti√®res qui maximisent la probabilit√© d'atteindre un but ou une r√©compense √©lev√©e.

---

## ü§ñ Applications et Workflow

### Exemples d'Applications
* **Manipulation Robotique (QT-Opt) :** Apprendre √† saisir des objets √† partir de mois de donn√©es collect√©es par plusieurs robots.
* **Navigation (BADGR / RECON) :** Apprendre √† naviguer en tout-terrain en utilisant des donn√©es collect√©es "hors ligne" (ex: vid√©os de conduite), en √©vitant les collisions et les terrains accident√©s.

### Le Workflow de l'Offline RL
Contrairement au cycle classique "Entra√Æner-Tester-Entra√Æner", l'Offline RL propose un workflow plus proche du Supervised Learning :
1.  **Collecte :** Accumuler un large dataset $\mathcal{D}$ (via des politiques al√©atoires, expertes, ou mixtes).
2.  **Entra√Ænement :** Apprendre une politique $\pi$ sur $\mathcal{D}$ (via CQL, IQL, DT, etc.).
3.  **√âvaluation (Le d√©fi) :** Comment savoir si la politique est bonne sans la tester sur le robot ?
    * L'√©valuation hors ligne (Off-Policy Evaluation - OPE) est difficile.
    * Souvent, on s√©lectionne les meilleurs mod√®les selon des m√©triques conservatrices (valeur Q moyenne p√©nalis√©e) avant de d√©ployer le meilleur candidat.

---

## ‚úÖ R√©sum√© des Architectures Avanc√©es

| Architecture | Principe | Avantages | Inconv√©nients |
| :--- | :--- | :--- | :--- |
| **MOPO / MOREL** (Model-Based) | P√©naliser la r√©compense par l'incertitude d'un ensemble de mod√®les. | G√©n√©ralise bien hors du dataset si la dynamique est apprenable. | Lourd (Ensemble), difficile pour les images complexes. |
| **Decision Transformer** (Sequence) | Conditionner l'action sur le retour d√©sir√© ($R_t$) via un Transformer. | Tr√®s stable (Supervised Learning), pas de probl√®mes de bootstrap/Q-values. | Ne peut pas "inventer" une strat√©gie meilleure que la meilleure trajectoire du dataset (pas de stitching optimal th√©orique). |
| **Conservative Q-Learning** (Model-Free) | Apprendre une Q-function pessimiste. | Souvent le plus performant pour "coudre" (stitch) des sous-trajectoires optimales. | Optimisation parfois instable, sensible aux hyperparam√®tres. |

---
*Source: CS 285 Lecture 16 Slides, Instructor: Sergey Levine, UC Berkeley.*